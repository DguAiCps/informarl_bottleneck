# InforMARL Bottleneck Environment - Claude Session Summary

## í”„ë¡œì íŠ¸ ê°œìš”

### í™˜ê²½ ì„¤ëª…
- **í”„ë¡œì íŠ¸ëª…**: InforMARL Bottleneck Environment
- **ëª©ì **: ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê°•í™”í•™ìŠµ í™˜ê²½ì—ì„œ ë³‘ëª© ì§€ì  í˜‘ë ¥ í•™ìŠµ
- **í•µì‹¬ ê¸°ìˆ **: UniMP GNN + PPO ê¸°ë°˜ í˜‘ë ¥ì  í•™ìŠµ
- **ì—ì´ì „íŠ¸**: 4ê°œ ì—ì´ì „íŠ¸ê°€ ë³‘ëª© ì§€ì ì„ í†µê³¼í•˜ë©° ëª©í‘œ ì§€ì  ë„ë‹¬

### í™˜ê²½ êµ¬ì„±ìš”ì†Œ
```
í™˜ê²½ í¬ê¸°: 20.0 Ã— 10.0 ë³µë„
ë³‘ëª© ìœ„ì¹˜: x=10.0, í­=6.0
ì—ì´ì „íŠ¸: ë°˜ì§€ë¦„ 0.5, ìµœëŒ€ì†ë„ 1.0
ì„¼ì‹± ë²”ìœ„: 7.0
ìµœëŒ€ ìŠ¤í…: 300
```

### í•µì‹¬ íŒŒì¼ êµ¬ì¡°
```
src/
â”œâ”€â”€ env/
â”‚   â”œâ”€â”€ bottleneck_env.py          # ë©”ì¸ í™˜ê²½
â”‚   â”œâ”€â”€ physics.py                 # ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜
â”‚   â”œâ”€â”€ reward.py                  # ë³´ìƒ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ map.py                     # ë§µ ìƒì„±
â”‚   â””â”€â”€ graph_builder_unimp.py     # GNN ê·¸ë˜í”„ ìƒì„±
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ policy_unimp.py            # Actor-Critic ëª¨ë¸
â”‚   â””â”€â”€ gnn_unimp.py               # UniMP GNN êµ¬í˜„
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ types.py                   # ë°ì´í„° íƒ€ì… ì •ì˜
â”‚   â””â”€â”€ rollout_buffer.py          # ë¡¤ì•„ì›ƒ ë²„í¼
â””â”€â”€ utils/
    â””â”€â”€ model_saver.py             # ëª¨ë¸ ì €ì¥/ë¡œë”©

main_stepwise.py                   # ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
```

## ë°œê²¬ëœ ì£¼ìš” ë¬¸ì œì ë“¤

### 1. ì‹¬ê°í•œ ë°ì´í„° ì¸ë±ì‹± ì˜¤ë¥˜ ğŸš¨
**ë¬¸ì œ**: í•˜ë‚˜ì˜ ì—ì´ì „íŠ¸ë§Œ í•™ìŠµë˜ê³  ë‚˜ë¨¸ì§€ëŠ” ê±°ì˜ í•™ìŠµë˜ì§€ ì•ŠìŒ

**ì›ì¸**: ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§ì—ì„œ ì˜ëª»ëœ ì—ì´ì „íŠ¸ ë¶„ë¥˜
```python
# ì˜ëª»ëœ ë°©ì‹
agent_start_idx = agent_id * samples_per_agent  # ì—°ì† ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ”
# ì‹¤ì œë¡œëŠ” ì„œë¡œ ë‹¤ë¥¸ íƒ€ì„ìŠ¤í…ì˜ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ ë°ì´í„°ê°€ ì„ì„
```

**ì‹¤ì œ ë°ì´í„° ìˆœì„œ**:
```
ë²„í¼ ì €ì¥: [step0_agent0, step0_agent1, step0_agent2, step0_agent3,
           step1_agent0, step1_agent1, step1_agent2, step1_agent3, ...]
ë¯¸ë‹ˆë°°ì¹˜: [ì¸ë±ìŠ¤ 847, 23, 456, 1089, ...] (ëœë¤)
```

### 2. ë¹ˆ í…ì„œ ë¬¸ì œ
**ë¬¸ì œ**: `torch.stack()` ì‹¤í–‰ ì‹œ í¬ê¸°ê°€ ë‹¤ë¥¸ í…ì„œë“¤ë¡œ ì¸í•œ ì˜¤ë¥˜
```
Agent 0: [16, 64] í¬ê¸° í…ì„œ
Agent 1: [15, 64] í¬ê¸° í…ì„œ
Agent 2: [17, 64] í¬ê¸° í…ì„œ
â†’ torch.stack() ì‹¤íŒ¨
```

### 3. ë…¼ë¬¸ ì˜ë„ ì™œê³¡
**ë¬¸ì œ**: Criticì´ ì‹œê°„ì  ì¼ê´€ì„± ì—†ëŠ” í˜¼í•© ë°ì´í„°ë¡œ í•™ìŠµ
- ì›ë˜: "ë™ì¼ ì‹œì  4ëª… í˜‘ë ¥ ìƒí™©ì˜ ê°€ì¹˜ í‰ê°€"
- ê¸°ì¡´ êµ¬í˜„: "ì„œë¡œ ë‹¤ë¥¸ ì‹œê°„ëŒ€ ì—ì´ì „íŠ¸ë“¤ì˜ í˜¼í•© ìƒí™© í‰ê°€"

## í•µì‹¬ í•´ê²°ì±…: Step-wise Sampling

### ê·¼ë³¸ì  í•´ê²° ë°©ë²•
**ê¸°ì¡´**: ê°œë³„ ì—ì´ì „íŠ¸ ë°ì´í„° ëœë¤ ìƒ˜í”Œë§
**ìˆ˜ì •**: Step ë‹¨ìœ„ ìƒ˜í”Œë§ (ë™ì¼ íƒ€ì„ìŠ¤í…ì˜ ëª¨ë“  ì—ì´ì „íŠ¸ í•¨ê»˜)

```python
class StepWiseSampler:
    def __init__(self, total_steps: int, num_agents: int, mini_batch_size: int):
        self.steps_per_batch = mini_batch_size // num_agents  # ìë™ ê³„ì‚°

    def __iter__(self):
        # 1. ìŠ¤í…ì„ ëœë¤ ì„ íƒ
        selected_steps = torch.randperm(self.total_steps)[:self.steps_per_batch]

        # 2. ê° ìŠ¤í…ì˜ ëª¨ë“  ì—ì´ì „íŠ¸ í¬í•¨
        indices = []
        for step in selected_steps:
            base_idx = step * self.num_agents
            for agent in range(self.num_agents):
                indices.append(base_idx + agent)
```

### í•´ê²°ëœ ë¬¸ì œë“¤

**1. âœ… ì¼ê´€ì„± ë¬¸ì œ í•´ê²°**
- ë™ì¼ ì‹œì ì˜ ëª¨ë“  ì—ì´ì „íŠ¸ê°€ í•¨ê»˜ ìƒ˜í”Œë§ë¨
- ë…¼ë¬¸ì˜ í˜‘ë ¥ì  í•™ìŠµ ì˜ë„ êµ¬í˜„

**2. âœ… ë¹ˆ í…ì„œ ë¬¸ì œ í•´ê²°**
- ê° ì—ì´ì „íŠ¸ë§ˆë‹¤ ì •í™•íˆ ë™ì¼í•œ ìˆ˜ì˜ ìƒ˜í”Œ (16ê°œì”©)
- `torch.stack()` ì˜¤ë¥˜ ë°œìƒ ë¶ˆê°€ëŠ¥

**3. âœ… ê· ë“±í•œ í•™ìŠµ**
- ëª¨ë“  ì—ì´ì „íŠ¸ê°€ ìì‹ ì˜ ë°ì´í„°ë¡œë§Œ í•™ìŠµ
- 4ëª… ëª¨ë‘ ë˜‘ê°™ì´ ë°œì „ ê°€ëŠ¥

## ìˆ˜ì •ëœ ì½”ë“œ êµ¬ì¡°

### 1. ìƒˆë¡œìš´ ìƒ˜í”ŒëŸ¬ (rollout_buffer.py)
```python
# ê¸°ì¡´ MiniBatchSampler â†’ StepWiseSamplerë¡œ êµì²´
class StepWiseSampler:
    # Step ë‹¨ìœ„ë¡œ ë¯¸ë‹ˆë°°ì¹˜ ìƒì„±
    # ë…¼ë¬¸ ì˜ë„ì— ë§ëŠ” ë™ì¼ íƒ€ì„ìŠ¤í… ì—ì´ì „íŠ¸ë“¤ì„ í•¨ê»˜ ìƒ˜í”Œë§
```

### 2. ë‹¨ìˆœí™”ëœ ì—ì´ì „íŠ¸ë³„ ì²˜ë¦¬ (main_stepwise.py)
```python
# GNN ì²˜ë¦¬ - ë³µì¡í•œ ë§ˆìŠ¤í‚¹ ì œê±°
for agent_id in range(NUM_AGENTS):
    agent_indices = torch.arange(agent_id, len(indices), NUM_AGENTS)
    agent_gnn_features = models.gnn(
        mini_batch['node_obs'][agent_indices],
        # ...
        agent_id
    )

# Actor ì²˜ë¦¬ - ë™ì¼í•œ ë°©ì‹
for agent_id in range(NUM_AGENTS):
    agent_indices = torch.arange(agent_id, len(indices), NUM_AGENTS)
    combined_input = torch.cat([
        mini_batch['local_obs'][agent_indices],
        all_agent_gnn_features[agent_id]
    ], dim=-1)
```

### 3. ë…¼ë¬¸ ì˜ë„ëŒ€ë¡œ Critic êµ¬í˜„
```python
# ê° ìŠ¤í…ë³„ë¡œ ëª¨ë“  ì—ì´ì „íŠ¸ì˜ GNN ì¶œë ¥ í‰ê· 
stacked_features = torch.stack(all_agent_gnn_features)    # [agents, steps, features]
avg_features_per_step = stacked_features.mean(dim=0)     # [steps, features]

# ê° ì—ì´ì „íŠ¸ì— í•´ë‹¹ ìŠ¤í…ì˜ ê¸€ë¡œë²Œ ì»¨í…ìŠ¤íŠ¸ ì ìš©
global_contexts = avg_features_per_step.repeat(NUM_AGENTS, 1)
new_values = models.critic(global_contexts)
```

## ì½”ë“œ ì‹¤í–‰ íë¦„

### 1. í™˜ê²½ ì´ˆê¸°í™”
```python
env = CleanBottleneckEnv(num_agents=4, ...)
models = InforMARLModels(...)
buffer = RolloutBuffer(buffer_size=300, num_agents=4, ...)
```

### 2. ë°ì´í„° ìˆ˜ì§‘ (ë§¤ ìŠ¤í…)
```python
for rollout_step in range(300):
    # 4ëª… ì—ì´ì „íŠ¸ ê°ê° ê´€ì¸¡
    local_obs_list = [agent0_obs, agent1_obs, agent2_obs, agent3_obs]

    # ê·¸ë˜í”„ ìƒì„± (ëª¨ë“  ì—ì´ì „íŠ¸ ê³ ë ¤)
    node_obs, adj, entity_types, edge_features = build_unimp_graph_observations(...)

    # ê° ì—ì´ì „íŠ¸ë³„ í–‰ë™ ì„ íƒ
    for agent_id in range(4):
        gnn_output = models.gnn(graph_data, agent_id)
        action_mean, action_std = models.actor(local_obs + gnn_output)

    # í™˜ê²½ ìŠ¤í… ì‹¤í–‰
    rewards = env.step(actions)

    # ë²„í¼ì— ì €ì¥
    buffer.store(local_obs_list, actions, rewards, ...)
```

### 3. í•™ìŠµ ì—…ë°ì´íŠ¸ (ë²„í¼ê°€ ê°€ë“ ì°° ë•Œ)
```python
# Step-wise ìƒ˜í”Œë§
sampler = StepWiseSampler(total_steps=300, num_agents=4, mini_batch_size=64)

for indices in sampler:  # 16ê°œ ìŠ¤í… Ã— 4ê°œ ì—ì´ì „íŠ¸ = 64ê°œ ìƒ˜í”Œ
    # 1. ì—ì´ì „íŠ¸ë³„ GNN ì²˜ë¦¬
    for agent_id in range(4):
        agent_indices = torch.arange(agent_id, 64, 4)  # [0,4,8,12,...]
        agent_gnn_features[agent_id] = models.gnn(data[agent_indices], agent_id)

    # 2. ì—ì´ì „íŠ¸ë³„ Actor ì²˜ë¦¬
    for agent_id in range(4):
        agent_indices = torch.arange(agent_id, 64, 4)
        actions[agent_id] = models.actor(obs[agent_indices] + gnn_features[agent_id])

    # 3. ë…¼ë¬¸ ì˜ë„ëŒ€ë¡œ Critic ì²˜ë¦¬
    step_wise_features = stack(agent_gnn_features).mean(dim=0)  # ê° ìŠ¤í…ë³„ í‰ê· 
    values = models.critic(step_wise_features.repeat(4, 1))     # ëª¨ë“  ì—ì´ì „íŠ¸ì— ì ìš©

    # 4. PPO ì†ì‹¤ ê³„ì‚° ë° ì—…ë°ì´íŠ¸
    policy_loss, value_loss, entropy_loss = compute_losses(...)
    optimizer.step()
```

## ì£¼ìš” ì„¤ì •ê°’

### í™˜ê²½ ì„¤ì •
```python
NUM_AGENTS = 4
ROLLOUT_LENGTH = 300
MINI_BATCH_SIZE = 64           # 4ì˜ ë°°ìˆ˜ í•„ìˆ˜
PPO_EPOCHS = 4
LEARNING_RATE = 3e-4
ENTROPY_COEF = 0.05           # 0.01ì—ì„œ ì¦ê°€ (ë” ë§ì€ íƒí—˜)
```

### ëª¨ë¸ êµ¬ì¡°
```python
LOCAL_OBS_DIM = 6             # [rel_pos, rel_vel, goal_pos]
ACTION_DIM = 2                # [dv, dw] ì†ë„ ë³€í™”ëŸ‰
GNN_OUTPUT_DIM = 64
ACTOR_HIDDEN_DIM = 64
CRITIC_HIDDEN_DIM = 64        # 256ìœ¼ë¡œ ì¦ê°€ ê³ ë ¤ ì¤‘
```

### ë¬¼ë¦¬ íŒŒë¼ë¯¸í„°
```python
MAX_SPEED = 1.0               # ìµœëŒ€ ì†ë„
AGENT_RADIUS = 0.5           # ì—ì´ì „íŠ¸ ë°˜ì§€ë¦„
SENSING_RADIUS = 7.0         # ì„¼ì‹± ë²”ìœ„
ì†ë„ ë³€í™”ìœ¨ = 0.5             # ë§¤ ìŠ¤í… ìµœëŒ€ Â±0.5 ì†ë„ ë³€í™”
ê°€ì†ë„ = 5.0 ë‹¨ìœ„/ì´ˆÂ²         # ìƒë‹¹íˆ ë¯¼ì²©í•œ ì›€ì§ì„
```

## ê·¸ë˜í”„ êµ¬ì¡° (UniMP)

### ë…¸ë“œ íƒ€ì…
```python
ENTITY_TYPES = {"agent": 0, "landmark": 1, "obstacle": 2}
```

### ë©”ì‹œì§€ íŒ¨ì‹± ê·œì¹™
- **Agent â†” Agent**: ì–‘ë°©í–¥ ì—°ê²°
- **Non-agent â†’ Agent**: ë‹¨ë°©í–¥ ì—°ê²° (landmark, obstacleì´ agentì—ê²Œ ì •ë³´ ì „ë‹¬)
- **Non-agent â†” Non-agent**: ì—°ê²° ì—†ìŒ

### ë…¸ë“œ íŠ¹ì„± (6ì°¨ì›)
```python
features = [rel_x, rel_y, rel_vx, rel_vy, goal_x, goal_y]
# ëª¨ë“  ìœ„ì¹˜ëŠ” ego agent ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œ
```

## ë³´ìƒ ì‹œìŠ¤í…œ

### ë³´ìƒ êµ¬ì„±ìš”ì†Œ
```python
# ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ (ìŒìˆ˜)
reward = -distance_to_goal

# ëª©í‘œ ë„ë‹¬ ë³´ìƒ (ì–‘ìˆ˜)
if distance < goal_radius:
    reward += 5.0

# ì¶©ëŒ íŒ¨ë„í‹°
if collision:
    reward -= collision_penalty
```

### ì •ê·œí™”
```python
# ì›ì‹œ ë³´ìƒ ë²”ìœ„: ì•½ [-23, 5]
# ì •ê·œí™” í›„: [0, 1] ë²”ìœ„ë¡œ ë³€í™˜
normalized_reward = (raw_reward + 20.0) / 20.0
```

## ì„±ëŠ¥ ì§€í‘œ

### ì¶”ì ë˜ëŠ” ë©”íŠ¸ë¦­
- **Success Rate**: ëª©í‘œ ë„ë‹¬ ì—ì´ì „íŠ¸ ë¹„ìœ¨
- **Episode Reward**: ì—í”¼ì†Œë“œë³„ ì´ ë³´ìƒ
- **Episode Length**: ì—í”¼ì†Œë“œ ì§€ì† ì‹œê°„
- **Collision Count**: ì¶©ëŒ ë°œìƒ íšŸìˆ˜

### ëª¨ë¸ ì €ì¥ ê¸°ì¤€
- **ìë™ ì €ì¥**: 50k ìŠ¤í…ë§ˆë‹¤
- **ìµœê³  ì„±ëŠ¥ ì €ì¥**: ì„±ê³µë¥  ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ëª¨ë¸ ì €ì¥
- **ìµœì¢… ì €ì¥**: í•™ìŠµ ì™„ë£Œ ì‹œ

## ë‹¤ìŒ í´ë¡œë“œ ì„¸ì…˜ì—ì„œ ì°¸ê³ ì‚¬í•­

### ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥
í˜„ì¬ ìˆ˜ì •ëœ ì½”ë“œëŠ” ëª¨ë“  ë¬¸ì œê°€ í•´ê²°ë˜ì–´ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### ì¶”ê°€ ê°œì„  ê³ ë ¤ì‚¬í•­
1. **CRITIC_HIDDEN_DIM**: 64 â†’ 256 ì¦ê°€ë¡œ Value Loss ê°ì†Œ ê³ ë ¤
2. **VALUE_LOSS_COEF**: 0.5 â†’ 0.25 ê°ì†Œë¡œ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ
3. **ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°**: ë‹¤ë¥¸ ë°°ìˆ˜ ì‹¤í—˜ (ì˜ˆ: 68, 72, 80 ë“±)

### ë””ë²„ê¹… ì •ë³´
- í•™ìŠµ ì´ˆê¸° ëª‡ ìŠ¤í…ì€ action_means, action_stds, rewardsê°€ ì¶œë ¥ë¨
- ì—…ë°ì´íŠ¸ ì´ˆê¸° 5íšŒëŠ” ì†ì‹¤ê°’ë“¤ì´ ìƒì„¸íˆ ì¶œë ¥ë¨
- ëª¨ë“  ì—ì´ì „íŠ¸ê°€ ê· ë“±í•˜ê²Œ í•™ìŠµë˜ëŠ”ì§€ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥

### ì£¼ì˜ì‚¬í•­
- `MINI_BATCH_SIZE`ëŠ” ë°˜ë“œì‹œ `NUM_AGENTS`ì˜ ë°°ìˆ˜ì—¬ì•¼ í•¨
- Step-wise samplingìœ¼ë¡œ ë…¼ë¬¸ì˜ í˜‘ë ¥ì  í•™ìŠµ ì˜ë„ê°€ êµ¬í˜„ë¨
- ëª¨ë“  í•˜ë“œì½”ë”©ì´ ìë™ ê³„ì‚° ë³€ìˆ˜ë¡œ êµì²´ë¨

ì´ì œ ëª¨ë“  ì—ì´ì „íŠ¸ê°€ í˜‘ë ¥ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ë³‘ëª© ì§€ì ì„ íš¨ìœ¨ì ìœ¼ë¡œ í†µê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€